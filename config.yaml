train:
  max_epochs: 30
  # learning_rate: 0.00005
  # mini_batch_size: 32
embedding:
  document_embedding:
    type: "DocumentRNNEmbeddings"
    # type: "DocumentLSTMEmbeddings"
    # type: "DocumentPoolEmbeddings"
    # type: "TransformerWordEmbeddings"
    config:
      # hidden_size: 512
      # hidden_size: 800
      rnn_type: "LSTM"
      hidden_size: 256
      rnn_layers: 2
      # pooling: "mean"
      # model: "/home/wokron/Code/Projects/kg-ace/model/bert-base-uncased"
      # layers: "all"
      # local_files_only: True
      # fine_tune: True
  token_embeddings:
    TransformerWordEmbeddings-1:
      model: "/home/wokron/Code/Projects/kg-ace/model/bert-base-uncased"
      layers: -1,-2,-3,-4
      pooling_operation: mean
    TransformerWordEmbeddings-2:
      model: bert-base-multilingual-cased # todo: need modify
      layers: -1,-2,-3,-4
      pooling_operation: mean
    # ELMoEmbeddings-0: # todo: need `pip install allennlp==0.9.0` but allennlp no longer support
    #   model: original
    CharacterEmbeddings:
      char_embedding_dim: 25
      hidden_size_char: 25
    WordEmbeddings-0:
      embeddings: "glove"
    WordEmbeddings-1:
      embeddings: en
    FlairEmbeddings-0:
      model: en-forward
    FlairEmbeddings-1:
      model: en-backward
    FlairEmbeddings-2:
      model: multi-forward
    FlairEmbeddings-3:
      model: multi-backward
    TransformerWordEmbeddings-0: 
      layers: '-1'
      pooling_operation: first
      model: xlm-roberta-large-finetuned-conll03-english # todo: need modify
model:
  dropout: 0.1
