train:
  max_epochs: 1
  learning_rate: 0.00005
  mini_batch_size: 32
embedding:
  document_embedding:
    # type: "DocumentRNNEmbeddings"
    # type: "DocumentLSTMEmbeddings"
    type: "DocumentPoolEmbeddings"
    # type: "TransformerWordEmbeddings"
    config:
      # hidden_size: 512
      # hidden_size: 800
      # rnn_type: "LSTM"
      pooling: "mean"
      # model: "/home/wokron/Code/Projects/kg-ace/model/bert-base-uncased"
      # layers: "all"
      # local_files_only: True
      # fine_tune: True
  token_embeddings:
    TransformerWordEmbeddings-1:
      model: "/home/wokron/Code/Projects/kg-ace/model/bert-base-uncased"
      layers: -1,-2,-3,-4
      pooling_operation: mean
    WordEmbeddings-0:
      embeddings: "glove"
    FlairEmbeddings-0:
      model: en-forward
    FlairEmbeddings-1:
      model: en-backward
    FlairEmbeddings-2:
      model: multi-forward
    FlairEmbeddings-3:
      model: multi-backward
    TransformerWordEmbeddings-0:
      model: "/home/wokron/Code/Projects/kg-ace/model/bert-base-uncased"
      layers: -1,-2
model:
  dropout: 0.1
